{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. Installing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip uninstall scikit-learn scikit-survival -y\n",
    "\n",
    "!pip install scikit-learn\n",
    "!pip install scikit-survival\n",
    "\n",
    "!pip install lifelines\n",
    "\n",
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sksurv\n",
    "import lifelines\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sksurv.linear_model import CoxnetSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "from sksurv.util import Surv\n",
    "\n",
    "import itertools\n",
    "\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import FitFailedWarning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_opt_EN(train_data, train_labels, \n",
    "                      l1_ratios=np.linspace(0.1, 1.0, 10), max_iter=100, alpha_min_ratio=0.01, cv_folds=5, verbose = True):\n",
    "   \n",
    "    \"\"\"\n",
    "    EN model hyperparam opt \n",
    "        - estimates alpha grid using initial model with l1 = 0.5\n",
    "        - 5-fold CV along alpha-lambda grid \n",
    "        - determines optimal alpha and lambda\n",
    "        - retrains model on whole training split using optimal settings\n",
    "        - returns model and CV results\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    labels_array = np.array([(status, time) for status, time in zip(train_labels.iloc[:, 0], train_labels.iloc[:, 1])], dtype=[('event', '?'), ('time', '<f8')])\n",
    "\n",
    "    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "    warnings.simplefilter(\"ignore\", FitFailedWarning)\n",
    "\n",
    "    print(\"estimating alphas with lambda=0.5...\")\n",
    "\n",
    "    initial_model = CoxnetSurvivalAnalysis(l1_ratio=0.5, alpha_min_ratio=alpha_min_ratio, max_iter=max_iter, n_alphas = 5)\n",
    "    initial_model.fit(train_data, labels_array)\n",
    "    estimated_alphas = initial_model.alphas_\n",
    "\n",
    "    print(f\"estimated {len(estimated_alphas)} alphas ranging from {estimated_alphas.min():.5f} to {estimated_alphas.max():.5f}.\")\n",
    "\n",
    "    #cv grid\n",
    "    param_grid = {\n",
    "        'l1_ratio': l1_ratios,\n",
    "        'alphas': [[alpha] for alpha in estimated_alphas]\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        CoxnetSurvivalAnalysis(max_iter=max_iter, fit_baseline_model = True),\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=1 if verbose else 0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(train_data, labels_array)\n",
    "\n",
    "    #get best model\n",
    "    best_model = grid_search.best_estimator_ \n",
    "    best_l1_ratio = grid_search.best_params_['l1_ratio']\n",
    "    best_alpha = grid_search.best_params_['alphas'][0]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nBest l1_ratio: {best_l1_ratio:.2f}, Best alpha: {best_alpha:.5f}\")\n",
    "\n",
    "    cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "    return best_model, cv_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_train_test(df_filtered, labels, testtrain_column='testtrain'):\n",
    "    \n",
    "    train_data = df_filtered[df_filtered[testtrain_column] == 'train'].drop(columns=[testtrain_column])\n",
    "    test_data = df_filtered[df_filtered[testtrain_column] == 'test'].drop(columns=[testtrain_column])\n",
    "\n",
    "    train_labels = labels[labels[testtrain_column] == 'train'].drop(columns=[testtrain_column])\n",
    "    test_labels = labels[labels[testtrain_column] == 'test'].drop(columns=[testtrain_column])\n",
    "\n",
    "    return train_data, test_data, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the directory containing the files\n",
    "dir = \"UKBRISK_ENModels/NHC/Initial_10y\"\n",
    "\n",
    "command = f\"dx ls {dir}\"\n",
    "files = subprocess.check_output(command, shell=True).decode(\"utf-8\").splitlines()\n",
    "\n",
    "\n",
    "# Filter the files to get only those that contain \"_combined_10y.csv\"\n",
    "absrisk_files = [f for f in files if \"_combined_10y.csv\" in f]\n",
    "\n",
    "# Download the files\n",
    "for file_name in absrisk_files:\n",
    "    download_cmd = f\"dx download {dir}/{file_name} --overwrite\"\n",
    "    subprocess.run(download_cmd, shell=True)\n",
    "\n",
    "merged_dataframes = {}\n",
    "\n",
    "# Process each file and merge dataframes\n",
    "for file_name in absrisk_files:\n",
    "    \n",
    "    abs_risk_data = pd.read_csv(file_name)\n",
    "\n",
    "    # Split the file name by underscores and extract parts\n",
    "    parts = file_name.split(\"_\")\n",
    "    \n",
    "    endpoint = parts[1]  # e.g., \"RD\"\n",
    "    \n",
    "    # The combination name is everything after the second element and before \"survival_probs\"\n",
    "    survival_index = parts.index(\"survival\")  # Find the index of \"survival\"\n",
    "    \n",
    "    # Construct the combination name\n",
    "    combo_name = \"_\".join(parts[2:survival_index])\n",
    "    \n",
    "    # Check if the endpoint already exists in the dictionary\n",
    "    if endpoint in merged_dataframes:\n",
    "        # Merge the new data with the existing data for this endpoint\n",
    "        merged_dataframes[endpoint] = pd.merge(\n",
    "            merged_dataframes[endpoint], \n",
    "            abs_risk_data[[\"eid\", \"survival_probability\", \"set\"]], \n",
    "            on=[\"eid\", \"set\"], \n",
    "            how=\"outer\"\n",
    "        )\n",
    "        # Rename the new survival probability column to reflect the combination\n",
    "        merged_dataframes[endpoint].rename(columns={\"survival_probability\": combo_name}, inplace=True)\n",
    "    else:\n",
    "        # Initialize the dataframe for this endpoint\n",
    "        abs_risk_data_merged = abs_risk_data[[\"eid\", \"set\", \"survival_probability\"]]\n",
    "        abs_risk_data_merged.rename(columns={\"survival_probability\": combo_name}, inplace=True)\n",
    "        merged_dataframes[endpoint] = abs_risk_data_merged\n",
    "\n",
    "print(\"All absolute risk data frames merged successfully.\")\n",
    "\n",
    "train_dataframes_absrisk_nhc_10y = {}\n",
    "test_dataframes_absrisk_nhc_10y = {}\n",
    "\n",
    "# Split each merged dataframe into train and test sets\n",
    "for endpoint in merged_dataframes:\n",
    "    df = merged_dataframes[endpoint]\n",
    "    \n",
    "    # Split based on the 'set' column\n",
    "    train_dataframes_absrisk_nhc_10y[endpoint] = df[df[\"set\"] == \"train\"].copy()\n",
    "    test_dataframes_absrisk_nhc_10y[endpoint] = df[df[\"set\"] == \"test\"].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#dl files\n",
    "dl_cmd = f\"dx download 'UKBRISK_Processed/Processed_final_25112024.tsv' --overwrite\"\n",
    "!{dl_cmd}\n",
    "df = pd.read_csv(\"Processed_final_25112024.tsv\", sep=\"\\t\")\n",
    "\n",
    "dl_cmd = f\"dx download 'Risk score dataframes/NHSHC_exclusion.tsv' --overwrite\"\n",
    "!{dl_cmd}\n",
    "exclusion = pd.read_csv(\"NHSHC_exclusion.tsv\", sep=\"\\t\")\n",
    "exclusion.rename(columns={'DM_at_base.x': 'DM_at_base'}, inplace=True)\n",
    "exclusion.rename(columns={'CKD_at_base': 'RD_at_base'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.3 Saving & Uploading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def upload_model(model, endpoint, combo_name, cvresults, directory=\"UKBRISK_ENModels/NHC/Secondary_10y\"):\n",
    "    \n",
    "    filename_model = f\"EN_{endpoint}_{combo_name}.pkl\"\n",
    "    upload_cmd_model = f\"dx upload {filename_model} --path {directory}/{filename_model}\"\n",
    "    \n",
    "    filename_cvresults = f\"EN_{endpoint}_{combo_name}_cvresults.tsv\"\n",
    "    upload_cmd_cvresults = f\"dx upload {filename_cvresults} --path {directory}/{filename_cvresults}\"\n",
    "    \n",
    "    joblib.dump(model, filename_model)\n",
    "    !{upload_cmd_model}\n",
    "    \n",
    "    cvresults.to_csv(filename_cvresults, sep='\\t', index=False)\n",
    "    !{upload_cmd_cvresults}\n",
    "    \n",
    "    os.remove(filename_model)\n",
    "    os.remove(filename_cvresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_and_upload_lps(model, train_data, test_data, train_labels, test_labels, endpoint, combo_name, directory=\"UKBRISK_ENModels/NHC/Secondary_10y\"):\n",
    "\n",
    "    train_lp = model.predict(train_data)\n",
    "    test_lp = model.predict(test_data)\n",
    "    \n",
    "    train_lp_df = pd.DataFrame({\"eid\": train_labels.index, \"LP\": train_lp})\n",
    "    test_lp_df = pd.DataFrame({\"eid\": test_labels.index, \"LP\": test_lp})\n",
    "\n",
    "    train_lp_filename = f\"{endpoint}_{combo_name}_train_LP.tsv\"\n",
    "    test_lp_filename = f\"{endpoint}_{combo_name}_test_LP.tsv\"\n",
    "    train_lp_df.to_csv(train_lp_filename, sep='\\t', index=False)\n",
    "    test_lp_df.to_csv(test_lp_filename, sep='\\t', index=False)\n",
    "    \n",
    "    upload_cmd_trainlp = f\"dx upload {train_lp_filename} --path {directory}/{train_lp_filename}\"\n",
    "    upload_cmd_testlp = f\"dx upload {test_lp_filename} --path {directory}/{test_lp_filename}\"\n",
    "    !{upload_cmd_trainlp}\n",
    "    !{upload_cmd_testlp}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_and_upload_coefficients(model, train_data, endpoint, combo_name, directory=\"UKBRISK_ENModels/NHC/Secondary_10y\"):\n",
    "\n",
    "    coeff_filename = f\"{endpoint}_{combo_name}_coefficients.tsv\"\n",
    "    coef_df = pd.DataFrame(model.coef_, index=train_data.columns, columns=[\"Coefficient\"])\n",
    "    coef_df.to_csv(coeff_filename, sep='\\t')\n",
    "    \n",
    "    upload_cmd_coef = f\"dx upload {coeff_filename} --path {directory}/{coeff_filename}\"\n",
    "    !{upload_cmd_coef}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_and_upload_survival_probs(best_model, train_data, test_data, endpoint, combo_name, directory=\"UKBRISK_ENModels/NHC/Secondary_10y\"):\n",
    "\n",
    "    unique_times = best_model.unique_times_\n",
    "    time_point_index = (np.abs(unique_times - 10)).argmin()\n",
    "    \n",
    "    print(f\"Selected time point index: {time_point_index}, Time: {unique_times[time_point_index]}\")\n",
    "\n",
    "    surv_probs_train = best_model.predict_survival_function(train_data, return_array=True)[:, time_point_index]\n",
    "\n",
    "    surv_probs_test = best_model.predict_survival_function(test_data, return_array=True)[:, time_point_index]\n",
    "\n",
    "    train_eid = train_data.index\n",
    "    test_eid = test_data.index\n",
    "\n",
    "    surv_10y_train_df = pd.DataFrame({\n",
    "        'eid': train_eid,\n",
    "        'survival_probability': surv_probs_train,\n",
    "        'set': 'train'\n",
    "    })\n",
    "\n",
    "    surv_10y_test_df = pd.DataFrame({\n",
    "        'eid': test_eid,\n",
    "        'survival_probability': surv_probs_test,\n",
    "        'set': 'test'\n",
    "    })\n",
    "\n",
    "    combined_df = pd.concat([surv_10y_train_df, surv_10y_test_df], ignore_index=True)\n",
    "\n",
    "    filename_combined = f\"EN_{endpoint}_{combo_name}_survival_probs_combined_10y.csv\"\n",
    "    combined_df.to_csv(filename_combined, index=False)\n",
    "    upload_cmd_combined = f\"dx upload {filename_combined} --path {directory}/{filename_combined}\"\n",
    "    subprocess.run(upload_cmd_combined, shell=True)\n",
    "    os.remove(filename_combined)\n",
    "    \n",
    "    print(f\"Combined survival probabilities at 10 years uploaded for {endpoint} - {combo_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.4 Define Predictors for Initial and Secondary models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#for secondary\n",
    "always_include = [\"clinicalrisk_Age.at.recruitment\", \"clinicalrisk_Sex_0\", \"clinicalrisk_Sex_1\", \"eid\", \"testtrain\"]\n",
    "\n",
    "predictor_combinations = {\n",
    "    \"nhc_pmh_ts\": [\"nhc\", \"pmh_\", \"ts_\"],\n",
    "    \"nhc_prs_metabolomics_pmh_ts\": [\"nhc\", \"prs_\", \"metabolomics_\", \"pmh_\", \"ts_\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#for initial\n",
    "risk_columns = [\n",
    "    \"pmh_ts\",\n",
    "    \"prs_metabolomics_pmh_ts\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Subset endpoints for NHS HC predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "endpoint_names_nhc = [\"DM\", \"CVD\", \"RD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 exclude based on NHC inclusion criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inclusion = exclusion[~exclusion.drop(columns=[\"eid\"]).any(axis=1)]['eid']\n",
    "print(exclusion.shape)\n",
    "print(inclusion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df = df[df['eid'].isin(inclusion)]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cols_with_nans = df.columns[df.isna().any()].tolist()\n",
    "cols_with_nans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 10y exclusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.loc[df[\"CVD_followup\"] > 10, \"CVD_status\"] = False\n",
    "df.loc[df[\"RD_followup\"] > 10, \"RD_status\"] = False\n",
    "df.loc[df[\"DM_followup\"] > 10, \"DM_status\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df[\"CVD_followup\"] = df[\"CVD_followup\"].clip(upper=10)\n",
    "df[\"RD_followup\"] = df[\"RD_followup\"].clip(upper=10)\n",
    "df[\"DM_followup\"] = df[\"DM_followup\"].clip(upper=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "status_mask = (\n",
    "    (df[\"CVD_status\"] == False) &\n",
    "    (df[\"RD_status\"] == False) &\n",
    "    (df[\"DM_status\"] == False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Final Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for risk_col in risk_columns:\n",
    "    print(f\"Processing population based on risk column: {risk_col}\")\n",
    "\n",
    "    # Determine the high-risk population for all endpoints based on survival probabilities (train data)\n",
    "    cvd_risk_filtered_5_train = train_dataframes_absrisk_nhc_10y[\"CVD\"].loc[\n",
    "        train_dataframes_absrisk_nhc_10y[\"CVD\"][risk_col] < 0.95, \"eid\"\n",
    "    ]\n",
    "    rd_risk_filtered_5_train = train_dataframes_absrisk_nhc_10y[\"RD\"].loc[\n",
    "        train_dataframes_absrisk_nhc_10y[\"RD\"][risk_col] < 0.95, \"eid\"\n",
    "    ]\n",
    "    dm_risk_filtered_5_train = train_dataframes_absrisk_nhc_10y[\"DM\"].loc[\n",
    "        train_dataframes_absrisk_nhc_10y[\"DM\"][risk_col] < 0.95, \"eid\"\n",
    "    ]\n",
    "\n",
    "    # Calculate intersections for the high-risk population at 5% threshold (train data)\n",
    "    high_risk_eids_5_train = set(cvd_risk_filtered_5_train).union(set(rd_risk_filtered_5_train), set(dm_risk_filtered_5_train))\n",
    "    print(f\"High-risk eids for 5% threshold (train): {len(high_risk_eids_5_train)}\")\n",
    "\n",
    "    cvd_risk_filtered_10_train = train_dataframes_absrisk_nhc_10y[\"CVD\"].loc[\n",
    "        train_dataframes_absrisk_nhc_10y[\"CVD\"][risk_col] < 0.90, \"eid\"\n",
    "    ]\n",
    "    rd_risk_filtered_10_train = train_dataframes_absrisk_nhc_10y[\"RD\"].loc[\n",
    "        train_dataframes_absrisk_nhc_10y[\"RD\"][risk_col] < 0.90, \"eid\"\n",
    "    ]\n",
    "    dm_risk_filtered_10_train = train_dataframes_absrisk_nhc_10y[\"DM\"].loc[\n",
    "        train_dataframes_absrisk_nhc_10y[\"DM\"][risk_col] < 0.90, \"eid\"\n",
    "    ]\n",
    "\n",
    "    # Calculate intersections for the high-risk population at 10% threshold (train data)\n",
    "    high_risk_eids_10_train = set(cvd_risk_filtered_10_train).union(set(rd_risk_filtered_10_train), set(dm_risk_filtered_10_train))\n",
    "    print(f\"High-risk eids for 10% threshold (train): {len(high_risk_eids_10_train)}\")\n",
    "\n",
    "    # Determine the high-risk population for all endpoints based on survival probabilities (test data)\n",
    "    cvd_risk_filtered_5_test = test_dataframes_absrisk_nhc_10y[\"CVD\"].loc[\n",
    "        test_dataframes_absrisk_nhc_10y[\"CVD\"][risk_col] < 0.95, \"eid\"\n",
    "    ]\n",
    "    rd_risk_filtered_5_test = test_dataframes_absrisk_nhc_10y[\"RD\"].loc[\n",
    "        test_dataframes_absrisk_nhc_10y[\"RD\"][risk_col] < 0.95, \"eid\"\n",
    "    ]\n",
    "    dm_risk_filtered_5_test = test_dataframes_absrisk_nhc_10y[\"DM\"].loc[\n",
    "        test_dataframes_absrisk_nhc_10y[\"DM\"][risk_col] < 0.95, \"eid\"\n",
    "    ]\n",
    "\n",
    "    # Calculate intersections for the high-risk population at 5% threshold (test data)\n",
    "    high_risk_eids_5_test = set(cvd_risk_filtered_5_test).union(set(rd_risk_filtered_5_test), set(dm_risk_filtered_5_test))\n",
    "    print(f\"High-risk eids for 5% threshold (test): {len(high_risk_eids_5_test)}\")\n",
    "\n",
    "    cvd_risk_filtered_10_test = test_dataframes_absrisk_nhc_10y[\"CVD\"].loc[\n",
    "        test_dataframes_absrisk_nhc_10y[\"CVD\"][risk_col] < 0.90, \"eid\"\n",
    "    ]\n",
    "    rd_risk_filtered_10_test = test_dataframes_absrisk_nhc_10y[\"RD\"].loc[\n",
    "        test_dataframes_absrisk_nhc_10y[\"RD\"][risk_col] < 0.90, \"eid\"\n",
    "    ]\n",
    "    dm_risk_filtered_10_test = test_dataframes_absrisk_nhc_10y[\"DM\"].loc[\n",
    "        test_dataframes_absrisk_nhc_10y[\"DM\"][risk_col] < 0.90, \"eid\"\n",
    "    ]\n",
    "\n",
    "    # Calculate intersections for the high-risk population at 10% threshold (test data)\n",
    "    high_risk_eids_10_test = set(cvd_risk_filtered_10_test).union(set(rd_risk_filtered_10_test), set(dm_risk_filtered_10_test))\n",
    "    print(f\"High-risk eids for 10% threshold (test): {len(high_risk_eids_10_test)}\")\n",
    "\n",
    "    # Proceed with model fitting for each endpoint...\n",
    "    for endpoint in endpoint_names_nhc:\n",
    "        print(f\"Started with endpoint: {endpoint}\")\n",
    "\n",
    "        # Endpoint specific exclusion and other preprocessing (same as before)\n",
    "        eids_to_include = df[df[f\"{endpoint}_at_base\"] == False][\"eid\"]\n",
    "        df_filtered = df[df[\"eid\"].isin(eids_to_include)]\n",
    "        print(f\"Retained n = {len(eids_to_include)} individuals due to criteria: past occurrence of endpoint\")\n",
    "\n",
    "        # Filter logical columns to improve robustness\n",
    "        logical_cols = df_filtered[[col for col in df_filtered.columns if (col.startswith('pmh_') or col.startswith('ts_')) and df_filtered[col].dtype == 'bool']]\n",
    "        cols_to_remove = [col for col in logical_cols.columns if logical_cols[col].mean() < 0.001 or logical_cols[col].mean() > 0.999]\n",
    "        df_filtered = df_filtered.drop(columns=cols_to_remove)\n",
    "\n",
    "        # Make labels\n",
    "        labels = df_filtered[[f\"{endpoint}_status\", f\"{endpoint}_followup\", \"eid\", \"testtrain\"]].copy()\n",
    "        labels = labels.set_index(\"eid\")\n",
    "\n",
    "        for combo_name, prefixes in predictor_combinations.items():\n",
    "            print(f\"Analyzing combination: {combo_name} for {risk_col} and endpoint {endpoint}\")\n",
    "\n",
    "            selected_cols = always_include + [col for col in df_filtered.columns if any(col.startswith(prefix) for prefix in prefixes) and col not in always_include]\n",
    "            df_filtered2 = df_filtered[selected_cols]\n",
    "            df_filtered2 = df_filtered2.set_index(\"eid\").replace({'TRUE': 1, 'FALSE': 0})\n",
    "\n",
    "            # Split the data into training and testing sets\n",
    "            train_data, test_data, train_labels, test_labels = split_train_test(df_filtered2, labels)\n",
    "\n",
    "            # Subset the train data further for high-risk individuals only\n",
    "            train_data_5 = train_data[train_data.index.isin(high_risk_eids_5_train)]\n",
    "            train_data_10 = train_data[train_data.index.isin(high_risk_eids_10_train)]\n",
    "\n",
    "            # Subset the test data further for high-risk individuals only\n",
    "            test_data_5 = test_data[test_data.index.isin(high_risk_eids_5_test)]\n",
    "            test_data_10 = test_data[test_data.index.isin(high_risk_eids_10_test)]\n",
    "\n",
    "            # Train and evaluate models using the high-risk population for 5% threshold\n",
    "            if not train_data_5.empty and not test_data_5.empty:\n",
    "                model_suffix_5pct = f\"risk_{risk_col}_model_{combo_name}_5pct\"\n",
    "                best_model_5, results_df_5 = train_opt_EN(train_data_5, train_labels.loc[train_data_5.index])\n",
    "                upload_model(best_model_5, endpoint, model_suffix_5pct, results_df_5)\n",
    "                save_and_upload_lps(best_model_5, train_data_5, test_data_5, train_labels.loc[train_data_5.index], test_labels.loc[test_data_5.index], endpoint, model_suffix_5pct)\n",
    "                save_and_upload_coefficients(best_model_5, train_data_5, endpoint, model_suffix_5pct)\n",
    "                calculate_and_upload_survival_probs(best_model_5, train_data_5, test_data_5, endpoint, model_suffix_5pct)\n",
    "\n",
    "            # Train and evaluate models using the high-risk population for 10% threshold\n",
    "            if not train_data_10.empty and not test_data_10.empty:\n",
    "                model_suffix_10pct = f\"risk_{risk_col}_model_{combo_name}_10pct\"\n",
    "                best_model_10, results_df_10 = train_opt_EN(train_data_10, train_labels.loc[train_data_10.index])\n",
    "                upload_model(best_model_10, endpoint, model_suffix_10pct, results_df_10)\n",
    "                save_and_upload_lps(best_model_10, train_data_10, test_data_10, train_labels.loc[train_data_10.index], test_labels.loc[test_data_10.index], endpoint, model_suffix_10pct)\n",
    "                save_and_upload_coefficients(best_model_10, train_data_10, endpoint, model_suffix_10pct)\n",
    "                calculate_and_upload_survival_probs(best_model_10, train_data_10, test_data_10, endpoint, model_suffix_10pct)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
