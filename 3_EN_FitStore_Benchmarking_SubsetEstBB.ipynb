{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. Installing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip uninstall scikit-learn scikit-survival -y\n",
    "\n",
    "!pip install scikit-learn\n",
    "!pip install scikit-survival\n",
    "\n",
    "!pip install lifelines\n",
    "\n",
    "!pip install joblib\n",
    "\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sksurv\n",
    "import lifelines\n",
    "\n",
    "import os \n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sksurv.linear_model import CoxnetSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "from sksurv.util import Surv\n",
    "\n",
    "import itertools\n",
    "\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import FitFailedWarning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_opt_EN(train_data, train_labels, \n",
    "                      l1_ratios=np.linspace(0.1, 1.0, 10), max_iter=100, alpha_min_ratio=0.01, cv_folds=5, verbose = True):\n",
    "   \n",
    "    \"\"\"\n",
    "    EN model hyperparam opt \n",
    "        - estimates alpha grid using initial model with l1 = 0.5\n",
    "        - 5-fold CV along a 10x10 alpha-lambda grid \n",
    "        - determines optimal alpha and lambda\n",
    "        - retrains model on whole training split using optimal settings\n",
    "        - returns model and CV results\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    labels_array = np.array([(status, time) for status, time in zip(train_labels.iloc[:, 0], train_labels.iloc[:, 1])], dtype=[('event', '?'), ('time', '<f8')])\n",
    "\n",
    "    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "    warnings.simplefilter(\"ignore\", FitFailedWarning)\n",
    "\n",
    "    print(\"estimating alphas with lambda=0.5...\")\n",
    "\n",
    "    initial_model = CoxnetSurvivalAnalysis(l1_ratio=0.5, alpha_min_ratio=alpha_min_ratio, max_iter=max_iter, n_alphas = 5)\n",
    "    initial_model.fit(train_data, labels_array)\n",
    "    estimated_alphas = initial_model.alphas_\n",
    "\n",
    "    print(f\"estimated {len(estimated_alphas)} alphas ranging from {estimated_alphas.min():.5f} to {estimated_alphas.max():.5f}.\")\n",
    "\n",
    "    #cv grid\n",
    "    param_grid = {\n",
    "        'l1_ratio': l1_ratios,\n",
    "        'alphas': [[alpha] for alpha in estimated_alphas]\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        CoxnetSurvivalAnalysis(max_iter=max_iter),\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=1 if verbose else 0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(train_data, labels_array)\n",
    "\n",
    "    #get best model\n",
    "    best_model = grid_search.best_estimator_ \n",
    "    best_l1_ratio = grid_search.best_params_['l1_ratio']\n",
    "    best_alpha = grid_search.best_params_['alphas'][0]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nBest l1_ratio: {best_l1_ratio:.2f}, Best alpha: {best_alpha:.5f}\")\n",
    "\n",
    "    cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "    return best_model, cv_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_train_test(df_filtered, labels, testtrain_column='testtrain'):\n",
    "    \n",
    "    train_data = df_filtered[df_filtered[testtrain_column] == 'train'].drop(columns=[testtrain_column])\n",
    "    test_data = df_filtered[df_filtered[testtrain_column] == 'test'].drop(columns=[testtrain_column])\n",
    "\n",
    "    train_labels = labels[labels[testtrain_column] == 'train'].drop(columns=[testtrain_column])\n",
    "    test_labels = labels[labels[testtrain_column] == 'test'].drop(columns=[testtrain_column])\n",
    "\n",
    "    return train_data, test_data, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#dl file\n",
    "dl_cmd = f\"dx download 'UKBRISK_Processed/Processed_final_25112024.tsv' --overwrite\"\n",
    "!{dl_cmd}\n",
    "df = pd.read_csv(\"Processed_final_25112024.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "endpoint_names = [\n",
    "    \"CVD\", \"HF\", \"CAD\", \"ISS\", \"PAD\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "endpoint_names = [\n",
    "    \"CVD\", \"HF\", \"BC\", \"DM\", \"LD\", \"RD\", \"AF\",  \"CAD\", \"VT\", \"ISS\", \n",
    "    \"AAA\", \"PAD\", \"AS\", \"COPD\", \"LC\", \"MEL\", \"CRC\", \"PC\",  \n",
    "    \"PD\", \"OP\", \"CAT\", \"POAG\", \"HT\", \"AD\" \n",
    "]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(endpoint_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subset ts for estbb questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dl_cmd = f\"dx download 'UKBRISK/Variables_to_calculate_risk_scores_v2_250621_includedcol.xlsx' --overwrite\"\n",
    "!{dl_cmd}\n",
    "mapping_EGCUT = pd.read_excel('Variables_to_calculate_risk_scores_v2_250621_includedcol.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "filemapping = 'Variables_to_calculate_risk_scores_v2_250621_includedcol.xlsx'\n",
    "included = []\n",
    "df_mapping = pd.read_excel(filemapping, sheet_name=3, usecols=[0,1])          \n",
    "included = df_mapping.loc[df_mapping.iloc[:,0]=='x', df_mapping.columns[1]].tolist() \n",
    "included = ['ts_' + var for var in dict.fromkeys(included)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "qrisk_cols = [col for col in df.columns if col.startswith('qrisk_') and col not in ['qrisk_Townsend.deprivation.index.at.recruitment', \n",
    "                                                                                    'qrisk_Illnesses.of.relatives.0_1', \n",
    "                                                                                    'qrisk_SBP_sd']]\n",
    "score2_cols = [col for col in df.columns if col.startswith('score_')]\n",
    "prevent_cols = [col for col in df.columns if col.startswith('prevent_') and col not in ['prevent_Townsend.deprivation.index.at.recruitment', \n",
    "                                                                                        'prevent_UACR',\n",
    "                                                                                        'prevent_Glycated.haemoglobin..HbA1c....Instance.0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "included += qrisk_cols + score2_cols + prevent_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "final_columns = [\n",
    "    col for col in df.columns\n",
    "    if col.startswith('pmh_')\n",
    "    or col in included\n",
    "    or col in [\"eid\", \"testtrain\"]\n",
    "    or any(col.startswith(ep) for ep in endpoint_names)\n",
    "]\n",
    "df = df[final_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.3 Saving & Uploading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def upload_model(model, endpoint, combo_name, cvresults, directory=\"UKBRISK_ENModels/ExtValEstBB\"):\n",
    "    \n",
    "    filename_model = f\"EN_{endpoint}_{combo_name}.pkl\"\n",
    "    upload_cmd_model = f\"dx upload {filename_model} --path {directory}/{filename_model}\"\n",
    "    \n",
    "    filename_cvresults = f\"EN_{endpoint}_{combo_name}_cvresults.tsv\"\n",
    "    upload_cmd_cvresults = f\"dx upload {filename_cvresults} --path {directory}/{filename_cvresults}\"\n",
    "    \n",
    "    joblib.dump(model, filename_model)\n",
    "    !{upload_cmd_model}\n",
    "    \n",
    "    cvresults.to_csv(filename_cvresults, sep='\\t', index=False)\n",
    "    !{upload_cmd_cvresults}\n",
    "    \n",
    "    os.remove(filename_model)\n",
    "    os.remove(filename_cvresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_and_upload_lps(model, train_data, test_data, train_labels, test_labels, endpoint, combo_name, directory=\"UKBRISK_ENModels/ExtValEstBB\"):\n",
    "\n",
    "    train_lp = model.predict(train_data)\n",
    "    test_lp = model.predict(test_data)\n",
    "    \n",
    "    train_lp_df = pd.DataFrame({\"eid\": train_labels.index, \"LP\": train_lp})\n",
    "    test_lp_df = pd.DataFrame({\"eid\": test_labels.index, \"LP\": test_lp})\n",
    "\n",
    "    train_lp_filename = f\"{endpoint}_{combo_name}_train_LP.tsv\"\n",
    "    test_lp_filename = f\"{endpoint}_{combo_name}_test_LP.tsv\"\n",
    "    train_lp_df.to_csv(train_lp_filename, sep='\\t', index=False)\n",
    "    test_lp_df.to_csv(test_lp_filename, sep='\\t', index=False)\n",
    "    \n",
    "    upload_cmd_trainlp = f\"dx upload {train_lp_filename} --path {directory}/{train_lp_filename}\"\n",
    "    upload_cmd_testlp = f\"dx upload {test_lp_filename} --path {directory}/{test_lp_filename}\"\n",
    "    !{upload_cmd_trainlp}\n",
    "    !{upload_cmd_testlp}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_and_upload_coefficients(model, train_data, endpoint, combo_name, directory=\"UKBRISK_ENModels/ExtValEstBB\"):\n",
    "\n",
    "    coeff_filename = f\"{endpoint}_{combo_name}_coefficients.tsv\"\n",
    "    coef_df = pd.DataFrame(model.coef_, index=train_data.columns, columns=[\"Coefficient\"])\n",
    "    coef_df.to_csv(coeff_filename, sep='\\t')\n",
    "    \n",
    "    upload_cmd_coef = f\"dx upload {coeff_filename} --path {directory}/{coeff_filename}\"\n",
    "    !{upload_cmd_coef}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.4 Predictor combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "always_include = ['qrisk_Age.at.recruitment','qrisk_Sex_0','qrisk_Sex_1', \"eid\", \"testtrain\"]\n",
    "\n",
    "predictor_combinations = {\n",
    "    \"agesex\": [],\n",
    "    \"pmh\": [\"pmh_\"],\n",
    "    \"ts\": [\"ts_\"],\n",
    "    #\"metabolomics\": [\"metabolomics_\"],\n",
    "    #\"prs\": [\"prs_\"],\n",
    "    #\"clinicalrisk\": [\"clinicalrisk_\"],\n",
    "    \"pmh_ts\": [\"pmh_\", \"ts_\"],\n",
    "    #\"prs_metabolomics\": [\"prs_\", \"metabolomics_\"],\n",
    "    #\"prs_metabolomics_pmh_ts\": [\"prs_\", \"metabolomics_\", \"pmh_\", \"ts_\"],\n",
    "    #\"clinicalrisk_pmh_ts\": [\"clinicalrisk_\", \"pmh_\", \"ts_\"],\n",
    "    #\"clinicalrisk_prs_metabolomics\": [\"clinicalrisk_\", \"prs_\", \"metabolomics_\"],\n",
    "    #\"everything\": [\"clinicalrisk_\", \"pmh_\", \"ts_\", \"prs_\", \"metabolomics_\"],\n",
    "    \"score\": [\"score_\"],\n",
    "    \"qrisk\": [\"qrisk_\"],\n",
    "    \"prevent\": [\"prevent_\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Final Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 for everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for endpoint in endpoint_names:\n",
    "    print(f\"started with: {endpoint}\")\n",
    "    \n",
    "    #endpoint specific exclusion\n",
    "    #bl endpoint status\n",
    "    eids_to_include = df[df[f\"{endpoint}_at_base\"] == False][\"eid\"]\n",
    "    df_filtered = df[df[\"eid\"].isin(eids_to_include)]\n",
    "    print(f\"retained n = {len(eids_to_include)} individuals due to criteria: past occurrence of endpoint\")\n",
    "    \n",
    "    #sex\n",
    "    if endpoint == \"PC\":\n",
    "        eids_to_exclude = df[df[\"clinicalrisk_Sex_0\"] == True][\"eid\"]\n",
    "        df_filtered = df_filtered[~df_filtered[\"eid\"].isin(eids_to_exclude)]\n",
    "    elif endpoint == \"BC\":\n",
    "        eids_to_exclude = df[df[\"clinicalrisk_Sex_1\"] == True][\"eid\"]\n",
    "        df_filtered = df_filtered[~df_filtered[\"eid\"].isin(eids_to_exclude)]\n",
    "        \n",
    "    #remove low count logical cols\n",
    "    logical_cols = df_filtered[[col for col in df_filtered.columns if (col.startswith('pmh_') or col.startswith('ts_')) and df_filtered[col].dtype == 'bool']]\n",
    "    cols_to_remove = [col for col in logical_cols.columns if logical_cols[col].mean() < 0.001 or logical_cols[col].mean() > 0.999]\n",
    "    df_filtered = df_filtered.drop(columns=cols_to_remove)\n",
    "    \n",
    "    #make labels\n",
    "    labels = df_filtered[[f\"{endpoint}_status\",f\"{endpoint}_followup\",\"eid\",\"testtrain\"]].copy()\n",
    "    labels = labels.set_index(\"eid\")\n",
    "\n",
    "    for combo_name, prefixes in predictor_combinations.items():\n",
    "        \n",
    "        print(f\"Analyzing combination: {combo_name}\")\n",
    "        \n",
    "        selected_cols = always_include + [col for col in df_filtered.columns if any(col.startswith(prefix) for prefix in prefixes) and col not in always_include]\n",
    "        df_filtered2 = df_filtered[selected_cols]\n",
    "        df_filtered2 = df_filtered2.set_index(\"eid\").replace({'TRUE': 1, 'FALSE': 0})\n",
    "\n",
    "        train_data, test_data, train_labels, test_labels = split_train_test(df_filtered2, labels)\n",
    "        \n",
    "        best_model, results_df = train_opt_EN(train_data, train_labels)\n",
    "        \n",
    "        upload_model(best_model, endpoint, combo_name, results_df)\n",
    "        \n",
    "        save_and_upload_lps(best_model, train_data, test_data, train_labels, test_labels, endpoint, combo_name)\n",
    "        \n",
    "        save_and_upload_coefficients(best_model, train_data, endpoint, combo_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Coef table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import subprocess, pandas as pd\n",
    "\n",
    "remote_dir = \"UKBRISK_ENModels/ExtValEstBB\"\n",
    "coef_dict = {}\n",
    "\n",
    "# grab list of all remote coeff files\n",
    "for fn in subprocess.check_output(f\"dx ls {remote_dir}\", shell=True, text=True).split():\n",
    "    if fn.endswith(\"_coefficients.tsv\"):\n",
    "        # download it\n",
    "        subprocess.run(f\"dx download {remote_dir}/{fn} --overwrite\", shell=True)\n",
    "        # read the single Coefficient column as a Series\n",
    "        s = pd.read_csv(fn, sep=\"\\t\", index_col=0)[\"Coefficient\"]\n",
    "        # name it by endpoint_combo\n",
    "        coef_dict[fn.replace(\"_coefficients.tsv\",\"\")] = s\n",
    "\n",
    "coef_table = pd.DataFrame(coef_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "coef_table.to_csv(\"all_endpoint_combo_coefficients.tsv\", sep=\"\\t\")\n",
    "upload_cmd = f\"dx upload 'all_endpoint_combo_coefficients.tsv' --path 'UKBRISK/all_endpoint_combo_coefficients.tsv'\"\n",
    "!{upload_cmd}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
