{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. Installing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip uninstall scikit-learn scikit-survival -y\n",
    "\n",
    "!pip install scikit-learn\n",
    "!pip install scikit-survival\n",
    "\n",
    "!pip install lifelines\n",
    "\n",
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sksurv\n",
    "import lifelines\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sksurv.linear_model import CoxnetSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "from sksurv.util import Surv\n",
    "\n",
    "import itertools\n",
    "\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import FitFailedWarning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_opt_EN(train_data, train_labels, \n",
    "                      l1_ratios=np.linspace(0.1, 1.0, 10), max_iter=100, alpha_min_ratio=0.01, cv_folds=5, verbose = True):\n",
    "   \n",
    "    \"\"\"\n",
    "    EN model hyperparam opt \n",
    "        - estimates alpha grid using initial model with l1 = 0.5\n",
    "        - 5-fold CV along alpha-lambda grid \n",
    "        - determines optimal alpha and lambda\n",
    "        - retrains model on whole training split using optimal settings\n",
    "        - returns model and CV results\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    labels_array = np.array([(status, time) for status, time in zip(train_labels.iloc[:, 0], train_labels.iloc[:, 1])], dtype=[('event', '?'), ('time', '<f8')])\n",
    "\n",
    "    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "    warnings.simplefilter(\"ignore\", FitFailedWarning)\n",
    "\n",
    "    print(\"estimating alphas with lambda=0.5...\")\n",
    "\n",
    "    initial_model = CoxnetSurvivalAnalysis(l1_ratio=0.5, alpha_min_ratio=alpha_min_ratio, max_iter=max_iter, n_alphas = 5)\n",
    "    initial_model.fit(train_data, labels_array)\n",
    "    estimated_alphas = initial_model.alphas_\n",
    "\n",
    "    print(f\"estimated {len(estimated_alphas)} alphas ranging from {estimated_alphas.min():.5f} to {estimated_alphas.max():.5f}.\")\n",
    "\n",
    "    #cv grid\n",
    "    param_grid = {\n",
    "        'l1_ratio': l1_ratios,\n",
    "        'alphas': [[alpha] for alpha in estimated_alphas]\n",
    "    }\n",
    "\n",
    "    cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        CoxnetSurvivalAnalysis(max_iter=max_iter, fit_baseline_model = True),\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=1 if verbose else 0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(train_data, labels_array)\n",
    "\n",
    "    #get best model\n",
    "    best_model = grid_search.best_estimator_ \n",
    "    best_l1_ratio = grid_search.best_params_['l1_ratio']\n",
    "    best_alpha = grid_search.best_params_['alphas'][0]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nBest l1_ratio: {best_l1_ratio:.2f}, Best alpha: {best_alpha:.5f}\")\n",
    "\n",
    "    cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "    return best_model, cv_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_train_test(df_filtered, labels, testtrain_column='testtrain'):\n",
    "    \n",
    "    train_data = df_filtered[df_filtered[testtrain_column] == 'train'].drop(columns=[testtrain_column])\n",
    "    test_data = df_filtered[df_filtered[testtrain_column] == 'test'].drop(columns=[testtrain_column])\n",
    "\n",
    "    train_labels = labels[labels[testtrain_column] == 'train'].drop(columns=[testtrain_column])\n",
    "    test_labels = labels[labels[testtrain_column] == 'test'].drop(columns=[testtrain_column])\n",
    "\n",
    "    return train_data, test_data, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#dl files\n",
    "dl_cmd = f\"dx download 'UKBRISK_Processed/Processed_final_27082024.tsv' --overwrite\"\n",
    "!{dl_cmd}\n",
    "df = pd.read_csv(\"Processed_final_27082024.tsv\", sep=\"\\t\")\n",
    "\n",
    "dl_cmd = f\"dx download 'Risk score dataframes/NHSHC_exclusion.tsv'\"\n",
    "!{dl_cmd}\n",
    "exclusion = pd.read_csv(\"NHSHC_exclusion.tsv\", sep=\"\\t\")\n",
    "exclusion.rename(columns={'DM_at_base.x': 'DM_at_base'}, inplace=True)\n",
    "exclusion.rename(columns={'CKD_at_base': 'RD_at_base'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.3 Saving & Uploading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def upload_model(model, endpoint, combo_name, cvresults, directory=\"UKBRISK_ENModels/NHC/Initial_10y\"):\n",
    "    \n",
    "    filename_model = f\"EN_{endpoint}_{combo_name}.pkl\"\n",
    "    upload_cmd_model = f\"dx upload {filename_model} --path {directory}/{filename_model}\"\n",
    "    \n",
    "    filename_cvresults = f\"EN_{endpoint}_{combo_name}_cvresults.tsv\"\n",
    "    upload_cmd_cvresults = f\"dx upload {filename_cvresults} --path {directory}/{filename_cvresults}\"\n",
    "    \n",
    "    joblib.dump(model, filename_model)\n",
    "    !{upload_cmd_model}\n",
    "    \n",
    "    cvresults.to_csv(filename_cvresults, sep='\\t', index=False)\n",
    "    !{upload_cmd_cvresults}\n",
    "    \n",
    "    os.remove(filename_model)\n",
    "    os.remove(filename_cvresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_and_upload_lps(model, train_data, test_data, train_labels, test_labels, endpoint, combo_name, directory=\"UKBRISK_ENModels/NHC/Initial_10y\"):\n",
    "\n",
    "    train_lp = model.predict(train_data)\n",
    "    test_lp = model.predict(test_data)\n",
    "    \n",
    "    train_lp_df = pd.DataFrame({\"eid\": train_labels.index, \"LP\": train_lp})\n",
    "    test_lp_df = pd.DataFrame({\"eid\": test_labels.index, \"LP\": test_lp})\n",
    "\n",
    "    train_lp_filename = f\"{endpoint}_{combo_name}_train_LP.tsv\"\n",
    "    test_lp_filename = f\"{endpoint}_{combo_name}_test_LP.tsv\"\n",
    "    train_lp_df.to_csv(train_lp_filename, sep='\\t', index=False)\n",
    "    test_lp_df.to_csv(test_lp_filename, sep='\\t', index=False)\n",
    "    \n",
    "    upload_cmd_trainlp = f\"dx upload {train_lp_filename} --path {directory}/{train_lp_filename}\"\n",
    "    upload_cmd_testlp = f\"dx upload {test_lp_filename} --path {directory}/{test_lp_filename}\"\n",
    "    !{upload_cmd_trainlp}\n",
    "    !{upload_cmd_testlp}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_and_upload_coefficients(model, train_data, endpoint, combo_name, directory=\"UKBRISK_ENModels/NHC/Initial_10y\"):\n",
    "\n",
    "    coeff_filename = f\"{endpoint}_{combo_name}_coefficients.tsv\"\n",
    "    coef_df = pd.DataFrame(model.coef_, index=train_data.columns, columns=[\"Coefficient\"])\n",
    "    coef_df.to_csv(coeff_filename, sep='\\t')\n",
    "    \n",
    "    upload_cmd_coef = f\"dx upload {coeff_filename} --path {directory}/{coeff_filename}\"\n",
    "    !{upload_cmd_coef}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_and_upload_survival_probs(best_model, train_data, test_data, endpoint, combo_name, directory=\"UKBRISK_ENModels/NHC/Initial_10y\"):\n",
    "\n",
    "    unique_times = best_model.unique_times_\n",
    "    time_point_index = (np.abs(unique_times - 10)).argmin()\n",
    "    \n",
    "    print(f\"Selected time point index: {time_point_index}, Time: {unique_times[time_point_index]}\")\n",
    "\n",
    "    surv_probs_train = best_model.predict_survival_function(train_data, return_array=True)[:, time_point_index]\n",
    "\n",
    "    surv_probs_test = best_model.predict_survival_function(test_data, return_array=True)[:, time_point_index]\n",
    "\n",
    "    train_eid = train_data.index\n",
    "    test_eid = test_data.index\n",
    "\n",
    "    surv_10y_train_df = pd.DataFrame({\n",
    "        'eid': train_eid,\n",
    "        'survival_probability': surv_probs_train,\n",
    "        'set': 'train'\n",
    "    })\n",
    "\n",
    "    surv_10y_test_df = pd.DataFrame({\n",
    "        'eid': test_eid,\n",
    "        'survival_probability': surv_probs_test,\n",
    "        'set': 'test'\n",
    "    })\n",
    "\n",
    "    combined_df = pd.concat([surv_10y_train_df, surv_10y_test_df], ignore_index=True)\n",
    "\n",
    "    filename_combined = f\"EN_{endpoint}_{combo_name}_survival_probs_combined_10y.csv\"\n",
    "    combined_df.to_csv(filename_combined, index=False)\n",
    "    upload_cmd_combined = f\"dx upload {filename_combined} --path {directory}/{filename_combined}\"\n",
    "    subprocess.run(upload_cmd_combined, shell=True)\n",
    "    os.remove(filename_combined)\n",
    "    \n",
    "    print(f\"Combined survival probabilities at 10 years uploaded for {endpoint} - {combo_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.4 Initial models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "always_include = [\"clinicalrisk_Age.at.recruitment\", \"clinicalrisk_Sex_0\", \"clinicalrisk_Sex_1\", \"eid\", \"testtrain\"]\n",
    "\n",
    "predictor_combinations = {\n",
    "    \"prs_metabolomics_pmh_ts\": [\"prs_\", \"metabolomics_\", \"pmh_\", \"ts_\"],\n",
    "    \"pmh_ts\": [\"pmh_\", \"ts_\"],\n",
    "    \"nhc\": [\"nhc_\"],\n",
    "    \"nhc_pmh_ts\": [\"nhc\", \"pmh_\", \"ts_\"],\n",
    "    \"nhc_prs_metabolomics_pmh_ts\": [\"nhc\", \"prs_\", \"metabolomics_\", \"pmh_\", \"ts_\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Subset endpoints for NHS HC predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "endpoint_names_nhc = [\"DM\", \"CVD\", \"RD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 exclude based on NHC inclusion criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inclusion = exclusion[~exclusion.drop(columns=[\"eid\"]).any(axis=1)]['eid']\n",
    "print(exclusion.shape)\n",
    "print(inclusion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df = df[df['eid'].isin(inclusion)]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 10y exclusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.loc[df[\"CVD_followup\"] > 10, \"CVD_status\"] = False\n",
    "df.loc[df[\"RD_followup\"] > 10, \"RD_status\"] = False\n",
    "df.loc[df[\"DM_followup\"] > 10, \"DM_status\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df[\"CVD_followup\"] = df[\"CVD_followup\"].clip(upper=10)\n",
    "df[\"RD_followup\"] = df[\"RD_followup\"].clip(upper=10)\n",
    "df[\"DM_followup\"] = df[\"DM_followup\"].clip(upper=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "status_mask = (\n",
    "    (df[\"CVD_status\"] == False) &\n",
    "    (df[\"RD_status\"] == False) &\n",
    "    (df[\"DM_status\"] == False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Final Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 for everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for endpoint in endpoint_names_nhc:\n",
    "    print(f\"started with: {endpoint}\")\n",
    "    \n",
    "    #endpoint specific exclusion\n",
    "    #bl endpoint status\n",
    "    eids_to_include = df[df[f\"{endpoint}_at_base\"] == False][\"eid\"]\n",
    "    df_filtered = df[df[\"eid\"].isin(eids_to_include)]\n",
    "    print(f\"retained n = {len(eids_to_include)} individuals due to criteria: past occurrence of endpoint\")\n",
    "    \n",
    "    logical_cols = df_filtered[[col for col in df_filtered.columns if (col.startswith('pmh_') or col.startswith('ts_')) and df_filtered[col].dtype == 'bool']]\n",
    "    cols_to_remove = [col for col in logical_cols.columns if logical_cols[col].mean() < 0.001 or logical_cols[col].mean() > 0.999]\n",
    "    df_filtered = df_filtered.drop(columns=cols_to_remove)\n",
    "    \n",
    "    #make labels\n",
    "    labels = df_filtered[[f\"{endpoint}_status\",f\"{endpoint}_followup\",\"eid\",\"testtrain\"]].copy()\n",
    "    labels = labels.set_index(\"eid\")\n",
    "\n",
    "    for combo_name, prefixes in predictor_combinations.items():\n",
    "        \n",
    "        print(f\"Analyzing combination: {combo_name}\")\n",
    "        \n",
    "        selected_cols = always_include + [col for col in df_filtered.columns if any(col.startswith(prefix) for prefix in prefixes) and col not in always_include]\n",
    "        df_filtered2 = df_filtered[selected_cols]\n",
    "        df_filtered2 = df_filtered2.set_index(\"eid\").replace({'TRUE': 1, 'FALSE': 0})\n",
    "\n",
    "        train_data, test_data, train_labels, test_labels = split_train_test(df_filtered2, labels)\n",
    "        \n",
    "        best_model, results_df = train_opt_EN(train_data, train_labels)\n",
    "        \n",
    "        upload_model(best_model, endpoint, combo_name, results_df)\n",
    "        \n",
    "        save_and_upload_lps(best_model, train_data, test_data, train_labels, test_labels, endpoint, combo_name)\n",
    "        \n",
    "        save_and_upload_coefficients(best_model, train_data, endpoint, combo_name)\n",
    "        \n",
    "        calculate_and_upload_survival_probs(best_model, train_data, test_data, endpoint, combo_name)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
